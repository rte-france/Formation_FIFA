{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 Prévision de consommation avec réseau de neurones\n",
    "\n",
    "\n",
    "<img src=\"pictures/Présentation_FormationIA_TPDeepLearning.png\" width=1000 >\n",
    "\n",
    "**Dans l'épisode précédent**  \n",
    "\n",
    "Nos modèles de régression du TP1 nous ont donné des premiers résultats et des premières intuitions sur notre problème de prévision de consommation pour le lendemain. \n",
    "\n",
    "Nous avons pu analyser des profils de courbe de consommation au jour, à la semaine, au mois. Nous avons également observé la dépendance entre la consommation et la consommation retardée. Nous avons aussi vu l'impact des jours fériés. \n",
    "\n",
    "Nous avons ensuite utilisé de premiers modèles en machine learning pour apprendre par observations l'influence de différents contextes sur la consommation sans les décrires explicitement selon des lois. Nous sommes arrivés à une erreur moyenne de test de 2,8%, bien mieux que ce qui avait été obtenu via une approche naïve.\n",
    "\n",
    "Des difficultés se sont posées pour intégrer les variables météorologiques très dépendantes entre elles et pour intégrer un vecteur de consommation retardée.\n",
    "\n",
    "Avec l'approche classique exposée dans ce TP1, nous avons en particulier constaté le besoin d'une expertise et d'un travail autour des variables explicatives pour obtenir un modèle performant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aujourd'hui** \n",
    "\n",
    "Nous allons de nouveau nous attaquer à ce sujet de la prévision de consommation nationale pour le lendemain, mais cette fois en utilisant un modèle de prévision par réseau de neurones. Nous allons exploiter leur capacité à capter ces phénomènes non-linéaires et interdépendants. Nous allons mettre en évidence le moindre besoin en feature engineering en travaillant directement à la granularité de la donnée, sans créer de variables agrégées ou transformées par de l'expertise.\n",
    "\n",
    "**Ce que vous allez voir dans ce second TP**\n",
    "\n",
    "- Un rappel de notre problème et récapitulatif des performances de nos modèles précédents\n",
    "- Une nouvelle méthode numérique pour préparer ses données et faciliter l'apprentissage : la normalisation\n",
    "- La création d'un premier réseau de neurones pour prédire la consommation dans 24h\n",
    "- L'utilisation de tensorboard pour observer en temps réel la courbe d'apprentissage du réseau de neurones\n",
    "- La création de modèles de plus en plus performants en intégrant davantage d'informations dans notre modélisation\n",
    "- L'évaluation des modèles sur 2 types de jeux de test\n",
    "\n",
    "**Ce que vous allez devoir faire**\n",
    "\n",
    "- Compléter les quelques trous de codes que nous vous avons laissé si vous le souhaitez. La solution est disponible dans le TP complété.\n",
    "- Répondre aux quelques questions disséminées dans ce TP\n",
    "- Entrainer votre propre modèle pour améliorer les performances d'un modèle existant et essayer de remporter notre mini-challenge !\n",
    "\n",
    "__NB__ : Pour ce TP nous utiliserons Keras, une bibliothèque python de haut niveau qui appelle des fonctions de la librairie TensorFlow. D'autres librairies existent, Keras a été retenue en raison de sa facilité d'utilisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionnement en temps\n",
    "La durée estimée de ce TP est d'environ 1h30 :\n",
    "- 10 minutes pour charger les données pour les réseaux de neurones \n",
    "- 20 minutes pour entrainer un premier modèle de réseau de neurones, en examiner le code implémentant ce réseau de neurones\n",
    "- Le reste pour jouer et tenter d'améliorer la qualité de la prédiction avec de nouvelles variables explicatives, ou en choisissant d'autres hyper-paramètres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÉchec du démarrage du noyau. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import zipfile\n",
    "import requests, io #get data from url\n",
    "from urllib.request import urlopen\n",
    "import joblib\n",
    "import tempfile\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, iplot_mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn est la librairie de machine learning en python et scipy une librairie statistiques\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Keras est la librairie que nous utilisons pour se créer des modèles de réseau de neurones\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Embedding, LSTM, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.utils as tf_utils\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import pydot\n",
    "import graphviz\n",
    "from IPython.display import display_png\n",
    "\n",
    "# set seed for rng\n",
    "tf_utils.set_random_seed(\n",
    "    42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "isInColab = True #False if in Jupyter\n",
    "\n",
    "if(isInColab):#to have iplot working in colab\n",
    "    import plotly.io as pio\n",
    "    #pio.renderers\n",
    "    pio.renderers.default = 'colab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Récupération et préparation des données\n",
    "\n",
    "Dans cette partie nous allons charger les fichiers csv nécessaires pour l'analyse, puis les convertir en data-frame python. Les données de base à récupérer sont :\n",
    "- la base de données issues du TP1 (Les historiques de consommation, leur lag, les données météo en température, leur lag, les jours feriés) \n",
    "\n",
    "En terme de transformation des données pour mieux les préparer:\n",
    "\n",
    "- nous allons aussi voir comment normaliser les données, une transformation souvent bien utile en pratique pour une meilleure convergence numérique. \n",
    "\n",
    "Cela vient compléter les transformations vu précédemment pour les données calendaires, et aussi la transformation \"one-hot\" pour les données catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "isDataFromGithub = False #True especially if using colab as it only download the notebook and not the entire github repository. Otherwise with Binder or in local, set it False\n",
    "if isInColab:\n",
    "    isDataFromGithub = True\n",
    "\n",
    "if(isDataFromGithub):\n",
    "    data_folder = 'https://raw.githubusercontent.com/rte-france/Formation_FIFA/master/data'#data depuis github\n",
    "else:\n",
    "    data_folder = os.path.join(os.getcwd(), \"data\") #data en local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération de nos variables à prédire: la consommation française"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "y_csv = os.path.join(data_folder, \"y_conso_tp2.csv\")\n",
    "y = pd.read_csv(y_csv, sep=\";\", engine='c', header=0)\n",
    "\n",
    "y['ds'] = pd.to_datetime(y['ds'], utc=True)\n",
    "\n",
    "display(y.head(5))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "* Petit rappel, que fait la fonction \"shape\" ?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données météo sont confidentielles, et donc ont été cryptées. Pour les lire vous avez besoin d'un mot de passe qui ne peut vous être donné que dans le cadre d'un travail au sein de RTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "password = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Récupération des températures et jours fériés\n",
    "x_zip = os.path.join(data_folder, \"x_input_tp2.zip\")\n",
    "\n",
    "if(isDataFromGithub):\n",
    "    ####data sur github\n",
    "    r = requests.get(x_zip)\n",
    "    x_zip_object = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "else:\n",
    "    ######data en local\n",
    "    x_zip_object = zipfile.ZipFile(x_zip)\n",
    "    \n",
    "x_zip_object.setpassword(bytes(password,'utf-8'))\n",
    "x = pd.read_csv(x_zip_object.open('x.csv'), sep=\";\", engine='c', header=0)\n",
    "\n",
    "x['ds'] = pd.to_datetime(x['ds'], utc=True)\n",
    "\n",
    "display(x.head(5))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# On récupère aussi le scaler qui permet de dénormaliser la prédiction du réseau de neurones\n",
    "if(isDataFromGithub):\n",
    "    scaler_conso_nat = joblib.load(urlopen(os.path.join(data_folder, \"scaler_conso.save\")))\n",
    "else:\n",
    "    scaler_conso_nat = joblib.load(os.path.join(data_folder, \"scaler_conso.save\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**A propos de la normalisation...**\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "En théorie, la normalisation des données d'entrée n'est pas indispensable pour entrainer un réseau de neurones.  \n",
    "\n",
    "En effet, on devrait apprendre des poids et biais plus ou moins importants pour équilibrer les contributions des différentes variables explicatives en entrée. \n",
    "\n",
    "Cependant en pratique, normaliser les données d'entrée permet généralement d'obtenir un apprentissage plus rapide du réseau de neurones.\n",
    "\n",
    "<br/>\n",
    "<font color='green'>\n",
    "    \n",
    "* Comment l'expliquez-vous ?\n",
    "\n",
    "<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Création des jeux d'apprentissage, de validation, et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "    \n",
    "**Question** : \n",
    "* A quoi servent les jeux d'entrainement, de validation, et de test ?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons nous créer les jeux de données suivants :\n",
    "* Jeu d'entrainement : 90% des points pris aléatoirement entre le début du dataset et le 31 décembre 2017\n",
    "* Jeu de validation : les 10% restant des points entre le début du dataset et le 31 décembre 2017\n",
    "* Jeu de test : tous les points horaires à partir du 1er janvier 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# D'abord on repère les lignes de chacun des set\n",
    "\n",
    "TEST_START_DATE = datetime.datetime(year=2018, month=1, day=1, tzinfo=datetime.timezone.utc)\n",
    "\n",
    "mask_test_set = (x[\"ds\"] >= TEST_START_DATE)\n",
    "\n",
    "mask_train_validation = (x['ds'] < TEST_START_DATE)\n",
    "mask_train_validation = mask_train_validation.astype(bool)\n",
    "\n",
    "def filter(value, threshold):\n",
    "    return True if value < threshold else False\n",
    "    \n",
    "mask_train_set = [filter(value, 0.9) for value in np.random.uniform(0, 1, size=x.shape[0])] & mask_train_validation\n",
    "mask_validation_set = ~mask_train_set & mask_train_validation\n",
    "\n",
    "# petite verif\n",
    "print(x.shape)\n",
    "print(\"Nombre d'éléments dans le train set : \" + str(np.sum(mask_train_set)))\n",
    "print(\"Nombre d'éléments dans le validation set : \" + str(np.sum(mask_validation_set)))\n",
    "print(\"Nombre d'éléments dans le test set : \" + str(np.sum(mask_test_set)))\n",
    "print(np.sum(mask_train_set) + np.sum(mask_validation_set) + np.sum(mask_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(x.columns)\n",
    "x.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Puis on constitue les sets\n",
    "x_train_full = x[mask_train_set]\n",
    "x_validation_full = x[mask_validation_set]\n",
    "x_test_full = x[mask_test_set]\n",
    "\n",
    "y_train_full = y[mask_train_set]\n",
    "y_validation_full = y[mask_validation_set]\n",
    "y_test_full = y[mask_test_set]\n",
    "\n",
    "x_train_full.reset_index(inplace=True, drop=True)\n",
    "x_validation_full.reset_index(inplace=True, drop=True)\n",
    "x_test_full.reset_index(inplace=True, drop=True)\n",
    "y_train_full.reset_index(inplace=True, drop=True)\n",
    "y_validation_full.reset_index(inplace=True, drop=True)\n",
    "y_test_full.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape de x_train_full : \" + str(x_train_full.shape))\n",
    "print(\"Shape de x_validation_full : \" + str(x_validation_full.shape))\n",
    "print(\"Shape de x_test_full : \" + str(x_test_full.shape))\n",
    "\n",
    "print(\"Shape de y_train : \" + str(y_train_full.shape))\n",
    "print(\"Shape de y_validation : \" + str(y_validation_full.shape))\n",
    "print(\"Shape de y_test : \" + str(y_test_full.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "y_light_columns = [\"conso_real_scaled\"]\n",
    "\n",
    "y_train_light = y_train_full[y_light_columns]\n",
    "y_validation_light = y_validation_full[y_light_columns]\n",
    "y_test_light = y_test_full[y_light_columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) Getting started with Keras API\n",
    "\n",
    "Jusqu'ici, nous avons importé nos données. Nous les avons ensuite préparées pour les fournir au réseau de neurones (one-hot encoding, normalisation). Nous avons également créé nos jeux d'entrainement, validation, et de test.\n",
    "\n",
    "Il est maintenant l'heure de se construire un réseau de neurones, de l'entrainer, et de lui faire faire des prédictions !\n",
    "\n",
    "Dans cette partie III) nous allons nous familiariser avec la librairie Keras qui permet d'implémenter des réseaux de neurones, puis en partie IV) nous l'appliquerons à notre problématique de prévision de consommation.\n",
    "\n",
    "**Cette partie III) est générique et indépendante de notre problématique de prévision de consommation**\n",
    "\n",
    "<img src=\"pictures/FirstNeuralNetwork.jpeg\" width=700 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deux fonctions bien utiles\n",
    "\n",
    "Nous allons commencer par implémenter deux fonctions que nous appellerons pour chacun des modèles que nous allons tester:\n",
    "- Fonction 1: **new_keras_model**, pour instancier un modèle de réseau de neurone avant apprentissage\n",
    "- Fonction 2: **plot_neural_net**, pour visualiser un réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une architecture de réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def new_keras_model(n_inputs, n_outputs=1, hidden_layers=None, activation='relu'):\n",
    "    \"\"\"      \n",
    "    arguments\n",
    "        - n_inputs : le nombre de features en entrée\n",
    "        - n_outputs : le nombre de sorties (variables à prédire)\n",
    "        - hidden_layers : une liste. \n",
    "                          La taille de la liste donne le nombre de couches cachées.\n",
    "                          Les éléments de la liste donnent le nombre de neurones par couche.\n",
    "                          Cette liste doit contenir au moins un élément\n",
    "        - activation: `str` \"relu\" ou \"sigmoid\" le type de \"non linéarité\" / \"fonction d'activation\"\n",
    "                      que vous voulez utiliser.\n",
    "        \n",
    "    returns\n",
    "        - un objet de type Model \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    input_dim = n_inputs\n",
    "    print(n_inputs)\n",
    "    for l_size in hidden_layers:\n",
    "        model.add(Dense(l_size, input_dim=input_dim, activation=activation))\n",
    "        input_dim = l_size\n",
    "\n",
    "    # Pour une régression, la fonction d'activation finale est simplement la fonction identité\n",
    "    model.add(Dense(n_outputs, input_dim=input_dim, activation='linear'))  \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection de l'architecture d'un reseau de neurones\n",
    "On se créé un réseau avec un certains nombre de couches qui peuvent chacune avoir différentes dimensions. On peut ensuite inspecter les dimensions et le nombre de paramètres de ce réseau avec la méthode _summary_ de Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# on se crée un réseau de neurones avec un certains nombre d'entrées et sorties\n",
    "n_inputs = 8  #un choix raisonnable pour visualiser ce modèle ensuite\n",
    "n_outputs = 1\n",
    "\n",
    "hidden_layers = [10, n_inputs, 6]\n",
    "dummy_model = new_keras_model(n_inputs, n_outputs, hidden_layers=hidden_layers)\n",
    "dummy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons-nous maintenant une fonction pour dessiner ce réseau de neurones.\n",
    "\n",
    "Ne vous embêtez pas trop à comprendre le code de la fonction *plot_neural_net*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_neural_net(model):\n",
    "    layers = [model.input_shape[1]]\n",
    "    for layer in model.layers:\n",
    "        layers.append(layer.get_output_at(0).get_shape().as_list()[1])\n",
    "        \n",
    "    tmp_file = os.path.join(tempfile.gettempdir(), 'out.dot')                \n",
    "    with open(tmp_file, 'w') as f:       \n",
    "        layers_str = [\"Input\"] + [\"Hidden\"] * (len(layers) - 2) + [\"Output\"]\n",
    "        layers_col = [\"none\"] + [\"none\"] * (len(layers) - 2) + [\"none\"]\n",
    "        layers_fill = [\"black\"] + [\"gray\"] * (len(layers) - 2) + [\"black\"]\n",
    "        penwidth = 15\n",
    "        font = \"Hilda 10\"\n",
    "        print(\"digraph G {\",file=f)\n",
    "        print(\"\\tfontname = \\\"{}\\\"\".format(font),file=f)\n",
    "        print(\"\\trankdir=LR\",file=f)\n",
    "        print(\"\\tsplines=line\",file=f)\n",
    "        print(\"\\tnodesep=.08;\",file=f)\n",
    "        print(\"\\tranksep=1;\",file=f)\n",
    "        print(\"\\tedge [color=black, arrowsize=.5];\",file=f)\n",
    "        print(\"\\tnode [fixedsize=true,label=\\\"\\\",style=filled,\" + \\\n",
    "              \"color=none,fillcolor=gray,shape=circle]\\n\",file=f)\n",
    "\n",
    "        # Clusters\n",
    "        for i in range(0, len(layers)):\n",
    "            print((\"\\tsubgraph cluster_{} {{\".format(i)),file=f)\n",
    "            print((\"\\t\\tcolor={};\".format(layers_col[i])),file=f)\n",
    "            print((\"\\t\\tnode [style=filled, color=white, penwidth={},\"\n",
    "                   \"fillcolor={} shape=circle];\".format(\n",
    "                penwidth,\n",
    "                layers_fill[i])),file=f)\n",
    "            print((\"\\t\\t\"), end=' ',file=f)\n",
    "            for a in range(layers[i]):\n",
    "                print(\"l{}{} \".format(i + 1, a), end=' ',file=f)\n",
    "\n",
    "            print(\";\",file=f)\n",
    "            print((\"\\t\\tlabel = {};\".format(layers_str[i])),file=f)\n",
    "            print(\"\\t}\\n\",file=f)\n",
    "\n",
    "        # Nodes\n",
    "        for i in range(1, len(layers)):\n",
    "            for a in range(layers[i - 1]):\n",
    "                for b in range(layers[i]):\n",
    "                    print(\"\\tl{}{} -> l{}{}\".format(i, a, i + 1, b), file=f)\n",
    "        print(\"}\", file=f)\n",
    "    \n",
    "    dot = graphviz.Source.from_file(tmp_file, engine='dot', format=\"png\")\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons le reseau de neurone test créé précédemment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plot_neural_net(dummy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION: Pour des grandes tailles de reseau, cette visualisation n'est pas adaptée et le temps d'éxécution de cette fonction sera très long !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "    \n",
    "**Défi !** : \n",
    "* Créez vous un reseau de neurones en forme de noeud papillon, dont la couche de sortie fait la même dimension que la couche d'entrée.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# votre new_keras_model à créer ici\n",
    "model_noeud_papillon = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#plot_neural_net(model_noeud_papillon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo ! Vous venez de créer un réseau de neurone d'une classe très particulière: c'est un autoencoder !\n",
    "Pour les curieux, vous pouvez retrouver le bestiaire des réseaux de neurones ici: https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Un premier modèle de réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix des variables explicatives\n",
    "\n",
    "pour ce TP, nous avons un jeu d'entrée X contenant beaucoup de variables. Afin de commencer par un modèle simple, nous allons élaguer ce X pour réduire le nombre de features en entrée. Dans ce TP, nous allons donc lister les colonnes à retirer des datasets X initialisés ci-dessus.\n",
    "\n",
    "Pour un cas d'étude réel, une approche pragmatique serait de commencer par se créer un premier X simple, de voir les performances du modèle, puis ensuite d'incorporer de plus en plus de features dans le X pour évaluer la progression des performances de nos modèles.\n",
    "\n",
    "Toutefois, en deep learning, il est courant commencer directement en mettant en entrée toute l'information disponible. En effet une des forces des réseaux de neurones est leur capacité à \"digérer\" la donnée, en se nourrissant d'informations redondantes.\n",
    "\n",
    "pour des raisons pédagogiques, nous allons commencer avec la première approche.\n",
    "\n",
    "Pour le premier réseau de neurones que nous allons entrainer, nous allons simplement garder les variables calendaires ainsi que la valeur de consommation nationale réalisée la veille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Petit rappel pour se remettre en mémoire les variables que nous avons à disposition\n",
    "x_train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sélectionnons un sous-ensemble de ces variables\n",
    "x_light_columns = [elt for elt in x.columns if elt not in \n",
    "                   [\"ds\", \"is_bank_holiday\", \"temperature_real_24h_avant\", \"temperature_prevue\", \"conso_real_24h_avant\"]\n",
    "                  ]\n",
    "\n",
    "x_train_light = x_train_full[x_light_columns]\n",
    "x_validation_light = x_validation_full[x_light_columns]\n",
    "x_test_light = x_test_full[x_light_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape de x_train_light : \" + str(x_train_light.shape))\n",
    "print(\"Shape de x_validation_light : \" + str(x_validation_light.shape))\n",
    "print(\"Shape de x_test_light : \" + str(x_test_light.shape))\n",
    "\n",
    "print(\"Shape de y_train_light : \" + str(y_train_light.shape))\n",
    "print(\"Shape de y_validation_light : \" + str(y_validation_light.shape))\n",
    "print(\"Shape de y_test_light : \" + str(y_test_light.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du réseau de neurones et hyper-paramétrage\n",
    "Un réseau de neurones profond est constuitué d'un certains nombre de couches, chacune portant un certain nombre de neurones. Ce sont 2 hyperparamètres que vous pouvez faire varier et qui vous permettront d'obtenir un apprentissage plus ou moins précis, en utilisant plus ou moins de puissance de calcul.\n",
    "\n",
    "Le \"learning rate\" de l'optimiseur est également un hyperparamètre qui influencera la convergence et la vitesse de convergence de l'apprentissage, où l'on cherche à optimiser notre modèle pour minimiser l'erreur de prédiction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_inputs = x_train_light.shape[1]  # nombre de features en entrée du réseau de neurones\n",
    "n_outputs = y_train_light.shape[1]\n",
    "hidden_layers = [n_inputs, n_inputs, n_inputs, n_inputs, n_inputs]\n",
    "\n",
    "first_model = new_keras_model(n_inputs, n_outputs, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# on affiche le nombre de paramètres de ce modèle avec la fonction summary de Keras\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "first_model.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer=Adam(lr=0.001), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# On crée ici une instance de l'utilitaire tensorboard qui va nous permettre de visualiser \n",
    "# les courbes d'apprentissage de nos différents modèles.\n",
    "# On reviendra avec plus d'explication sur TensorBoard un peu plus tard.\n",
    "# Donner un nom a votre modele pour le retrouver dans les logs tensorboard\n",
    "model_name = \"my_first_model_\" + datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement\n",
    "\n",
    "La cellule suivante peut prendre un peu de temps à s'exécuter. On reconnait là la méthode **fit** commune à chaque modèle de machine-learning pour entraîner son modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres d'appel\n",
    "# - epoch: on précise le nombre d'epochs (le nombre de fois que l'on voit le jeu d'apprentissage en entier)\n",
    "# - batch size: le nombre d'exemples sur lequel on fait un \"pas\" d'apprentissage parmi tout le jeu\n",
    "# - validation_split: la proportion d'exemples que l'on conserve pour notre jeu de validation\n",
    "# - callbacks: pour appeler des utilitaires/fonctions externes pour récupérer des résultats\n",
    "first_model.fit(\n",
    "    x_train_light, \n",
    "    y_train_light, \n",
    "    epochs=100, \n",
    "    batch_size=100, \n",
    "    validation_data=(x_validation_light, y_validation_light),\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "* D'après les informations de logs exposées ici, quelle semble être la perfomance atteinte par votre réseau de neurones ? \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "c'est un utilitaire de tensorflow qui permet de visualiser en temps réel les courbes d'apprentissage des réseau de neurones et est donc utile pour arrêter l'apprentissage si les progrès sont faibles.\n",
    "\n",
    "En particulier, vous pouvez vous intéresser à la courbe de l'erreur (loss) d'entrainement et de validation pour visualiser la progression de l'apprentissage et une tendance au surapprentissage en fin d'apprentissage.\n",
    "\n",
    "<img src=\"pictures/CourbesTensorboard.png\" width=1000 >\n",
    "\n",
    "**Pour ouvrir une fenêtre tensorboard, revenez sur la page d'accueil de Jupyter, placez vous dans le dossier logs dans lequel se trouve les logs de vos entrainement, puis cliquez sur New (en haut à droite) et enfin sur Tensorboard**.\n",
    "\n",
    "Une fenêtre pop-up doit s'ouvrir. Si elle est bloquée, autorisez son ouverture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#or you can directly load tensorboard in the notebook - especially if you are on Colab and not in Jupyter\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "* Vous devriez visualiser les courbes de 2 modèles: celui que vous venez d'entrainer et un modèle qui avait été entrainé de la même manière mais avec des données non normalisée. Que constatez-vous ? Comment l'expliquez-vous ?\n",
    "<br/><br/>\n",
    "* Il se passe quelque chose d'étonnant vers l'epoch 50. Qu'est-ce que cela vous inspire ?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation de la qualité du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "predictions_train_scaled = first_model.predict(x_train_light)\n",
    "predictions_val_scaled = first_model.predict(x_validation_light)\n",
    "predictions_test_scaled = first_model.predict(x_test_light)\n",
    "\n",
    "predictions_train = scaler_conso_nat.inverse_transform(predictions_train_scaled).reshape(-1)\n",
    "predictions_val = scaler_conso_nat.inverse_transform(predictions_val_scaled).reshape(-1)\n",
    "predictions_test = scaler_conso_nat.inverse_transform(predictions_test_scaled).reshape(-1)\n",
    "\n",
    "print(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "relative_error_on_train = np.abs((y_train_full['conso_real'] - predictions_train) / y_train_full['conso_real'])\n",
    "mean_error_on_train = np.mean(relative_error_on_train)\n",
    "max_error_on_train = np.max(relative_error_on_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_train_full['conso_real'], predictions_train))\n",
    "\n",
    "print(\"Erreur moyenne sur le jeu de train : \" + str(mean_error_on_train * 100) + \" %\")\n",
    "print(\"Erreur max sur le jeu de train : \" + str(max_error_on_train * 100) + \" %\")\n",
    "print(\"RMSE : \" + str(rmse) + \" MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "relative_error_on_val = np.abs((y_validation_full['conso_real'] - predictions_val) / y_validation_full['conso_real'])\n",
    "mean_error_on_val = np.mean(relative_error_on_val)\n",
    "max_error_on_val = np.max(relative_error_on_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_validation_full['conso_real'], predictions_val))\n",
    "\n",
    "print(\"Erreur moyenne sur le jeu de validation : \" + str(mean_error_on_val * 100) + \" %\")\n",
    "print(\"Erreur max sur le jeu de validation : \" + str(max_error_on_val * 100) + \" %\")\n",
    "print(\"RMSE : \" + str(rmse) + \" MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "relative_error_on_test = np.abs((y_test_full['conso_real'] - predictions_test) / y_test_full['conso_real'])\n",
    "mean_error_on_test = np.mean(relative_error_on_test)\n",
    "max_error_on_test = np.max(relative_error_on_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_full['conso_real'], predictions_test))\n",
    "\n",
    "print(\"Erreur moyenne sur le jeu de test : \" + str(mean_error_on_test * 100) + \" %\")\n",
    "print(\"Erreur max sur le jeu de test : \" + str(max_error_on_test * 100) + \" %\")\n",
    "print(\"RMSE : \" + str(rmse) + \" MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "iplot([{\"x\": y_test_full['ds'], \"y\": y_test_full['conso_real'], \"name\": \"realise\"},\n",
    "       {\"x\": y_test_full['ds'], \"y\": predictions_test, \"name\": \"prevision\"}\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur est ici comparable à celle des autres modèles en machine Learning (random forest, xgboost). Cela peut nous conforter dans le fait que notre réseau de neurones s'est créé de bonnes représentations pour ces variables calendaires. \n",
    "\n",
    "La différence en performance peut devenir plus flagrante lorsque l'on intègre des variables à une maille très granulaire (les pixels d'une images, la température dans toutes les villes de France) avec une forte interdépendance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour inspecter dynamiquement des visualisations, la librairie plotly se révèle très utile.\n",
    "Ci-dessous vous pouvez identifier les jours et heures qui présentent les erreurs les plus importantes pour ensuite imaginer ce qui a pu pêcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "iplot([{\"x\": y_test_full['ds'], \"y\": relative_error_on_test}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "* Quelles sont les heures ou les journées avec les erreurs les plus importantes. Avez-vous une idée à quoi pourrait correspondre ces heures ou ces jours ?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V) A vous de jouer, faites fonctionner vos neurones naturels\n",
    "\n",
    "<img src=\"pictures/we-need-you.png\" width=500 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Challenge**: entrainez et testez votre nouveau modèle avec de nouvelles variables et paramètres choisies\n",
    "\n",
    "N'hésitez pas à largement copier-coller des morceaux de code ci-dessus ;-)  \n",
    "Venez partager vos investigations sur cette google sheet : https://docs.google.com/spreadsheets/d/1oIx8jjzIh7Ugp3ZJMCOEwns6KCJxo4ua_jW5hIvjjFI/edit?usp=sharing\n",
    "\n",
    "Quelques idées si vous n'êtes pas inspirés :\n",
    "- essayer d'autres hyperparamètres (learning rate, taille des minibatch, nombre de couches...)\n",
    "- regarder ce qu'il se passe si on utilise des variables non normalisées\n",
    "- ajouter d'autres variables en entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Votre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rappel des variables explicatives à disposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initialement\n",
    "x_train_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix des variables explicatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sélectionne les variables que l'on souhaite conserver en précisant simplement à quelle catégorie elles appartiennent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sélectionnons un sous-ensemble de ces variables\n",
    "\n",
    "######### TO DO #########\n",
    "\n",
    "# Prenez ce que vous voulez dans \"x_light_columns\"\n",
    "x_light_columns = [elt for elt in x.columns if elt not in \n",
    "                   [\"ds\", \"temperature_real_24h_avant\", \"temperature_prevue\", \"conso_real_24h_avant\"]\n",
    "                  ]\n",
    "#########################\n",
    "\n",
    "x_train_light = x_train_full[x_light_columns]\n",
    "x_validation_light = x_validation_full[x_light_columns]\n",
    "x_test_light = x_test_full[x_light_columns]\n",
    "\n",
    "print(x_train_light.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du réseau de neurones, hyper-paramétrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez jouer sur l'architecture de votre reseau de neurones ici en précisant le nombre de couches et la taille des couches dans le vecteur hiddenLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_inputs = x_train_light.shape[1]  # nombre d'entrées du modèle\n",
    "n_outputs = y_train_light.shape[1]\n",
    "\n",
    "######### TO DO #########\n",
    "# votre choix  (nombre de couche et taille des couches)\n",
    "hidden_layers = [n_inputs, n_inputs, n_inputs, n_inputs, n_inputs]\n",
    "\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# nombre d'epoch (= nombre de fois ou chaque element du jeu de données sera utilisé pour \"apprendre\")\n",
    "nb_epochs = 100\n",
    "\n",
    "# batch size (= nombre de lignes de la base de données qui seront utilisées pour calculer \n",
    "# les gradients lors d'une iteration d'apprentissage)\n",
    "batch_size = 64\n",
    "#########################\n",
    "\n",
    "# NB: le nombre total \"d'iteration de descente de gradient\" est donc\n",
    "# (taille base apprentissage / batch_size) * nb_epochs\n",
    "\n",
    "mon_reseau_de_neurones = new_keras_model(n_inputs, n_outputs, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# on affiche le nombre de paramètres de votre modèle avec la fonction \"summary\" de Keras\n",
    "mon_reseau_de_neurones.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mon_reseau_de_neurones.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer=Adam(lr=lr),  # <=  TODO : vous pouvez jouer avec ça aussi\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "Notre utilitaire de tensorflow qui permet de visualiser en temps réel les courbes d'apprentissage des réseau de neurones et est donc utile pour arrêter l'apprentissage si les progrès sont faibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Donner un nom a votre modele pour le retrouver dans les logs tensorboard\n",
    "model_name = \"my_own_model_\" + datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(model_name),histogram_freq=1)\n",
    "\n",
    "\n",
    "#lancement dans le notebook\n",
    "#vous pouvez rafraîchir tensorboard lorsque un modèle est entraîné pour voir la progression tensorboard \n",
    "%tensorboard --logdir logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement\n",
    "\n",
    "La cellule suivante peut prendre un peu de temps à s'exécuter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mon_reseau_de_neurones.fit(\n",
    "    x_train_light, \n",
    "    y_train_light, \n",
    "    epochs=nb_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_validation_light, y_validation_light),\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation de la qualité du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "predictions_train_scaled = mon_reseau_de_neurones.predict(x_train_light)\n",
    "predictions_train = scaler_conso_nat.inverse_transform(predictions_train_scaled).reshape(-1)\n",
    "\n",
    "predictions_val_scaled = mon_reseau_de_neurones.predict(x_validation_light)\n",
    "predictions_val = scaler_conso_nat.inverse_transform(predictions_val_scaled).reshape(-1)\n",
    "\n",
    "predictions_test_scaled = mon_reseau_de_neurones.predict(x_test_light)\n",
    "predictions_test = scaler_conso_nat.inverse_transform(predictions_test_scaled).reshape(-1)\n",
    "\n",
    "print(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "iplot([{\"x\": y_test_full['ds'], \"y\": y_test_full['conso_real'], \"name\": \"realise\"},\n",
    "       {\"x\": y_test_full['ds'], \"y\": predictions_test, \"name\": \"prevision\"}\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "relative_error_on_train = np.abs((y_train_full['conso_real'] - predictions_train) / y_train_full['conso_real'])\n",
    "mean_error_on_train = np.mean(relative_error_on_train)\n",
    "max_error_on_train = np.max(relative_error_on_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_train_full['conso_real'], predictions_train))\n",
    "\n",
    "print(\"Erreur moyenne sur le jeu de train : \" + str(mean_error_on_train * 100) + \" %\")\n",
    "print(\"Erreur max sur le jeu de train : \" + str(max_error_on_train * 100) + \" %\")\n",
    "print(\"RMSE : \" + str(rmse) + \" MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "relative_error_on_val = np.abs((y_validation_full['conso_real'] - predictions_val) / y_validation_full['conso_real'])\n",
    "mean_error_on_val = np.mean(relative_error_on_val)\n",
    "max_error_on_val = np.max(relative_error_on_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_validation_full['conso_real'], predictions_val))\n",
    "\n",
    "print(\"Erreur moyenne sur le jeu de validation : \" + str(mean_error_on_val * 100) + \" %\")\n",
    "print(\"Erreur max sur le jeu de validation : \" + str(max_error_on_val * 100) + \" %\")\n",
    "print(\"RMSE : \" + str(rmse) + \" MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "relative_error_on_test = np.abs((y_test_full['conso_real'] - predictions_test) / y_test_full['conso_real'])\n",
    "\n",
    "mean_error_on_test = np.mean(relative_error_on_test)\n",
    "max_error_on_test = np.max(relative_error_on_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_full['conso_real'], predictions_test))\n",
    "\n",
    "print(\"Erreur moyenne sur le jeu de test : \" + str(mean_error_on_test * 100) + \" %\")\n",
    "print(\"Erreur max sur le jeu de test : \" + str(max_error_on_test * 100) + \" %\")\n",
    "print(\"RMSE : \" + str(rmse) + \" MW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "iplot([{\"x\": y_test_full['ds'], \"y\": 100. * relative_error_on_test, \"name\": \"erreur relative (%)\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller encore plus loin\n",
    "\n",
    "Le modèle ci-dessus peut être rendu encore plus performant par exemple en considérant des features comme \"jour d'avant vacances\", \"jour d'après vacances\"... \n",
    "\n",
    "Passer du temps à tuner les hyper-paramètres serait certainement bénéfique aussi.\n",
    "\n",
    "De manière assez surprenante, élargir le réseau de neurones pour prédire les consommations régionales peut également améliorer la qualité de la prédiction de l'échelle nationale. C'est l'idée du multi-tasking. Pour intégrer ces données supplémentaire, il est nécessaire de passer un peu de temps pour repréparer les données : aussi rendez-vous dans le TP *preparation_donnees.ipynb*.\n",
    "\n",
    "On pourra également considérer en sortie du modèle non pas la prédiction pour juste 24 heures plus tard, mais plutôt pour une plage horaire [1 heure plus tard, ..., 24 heures plus tard]. Ceci permet de capter des dynamiques. Un réseau de neurones de type convolutionnel serait aussi une option crédible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "fiabilite_materiel",
   "language": "python",
   "name": "fiabilite_materiel"
  },
  "vscode": {
   "interpreter": {
    "hash": "efaa606f95d5c0dd283242e9c23d7a9db7180ec698fc9a1b3f76fa1be9392c1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
