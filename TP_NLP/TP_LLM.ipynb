{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEJqSwyKyatS"
      },
      "source": [
        "# **TP LLM : Large Language Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEg4xb_zSTaE"
      },
      "source": [
        "## **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6ysGrYQyibi"
      },
      "source": [
        "Les **LLMs (Large Language Models)** ont r√©volutionn√© le domaine de l'Intelligence Artificielle en permettant de r√©aliser des t√¢ches jusqu'ici consid√©r√©es complexes : chatbots, traduction automatique, r√©sum√© de textes, g√©n√©ration de code...\n",
        "\n",
        "D√©mocratis√©s par **OpenAI** en novembre 2022 avec **ChatGPT**, les LLMs sont rapidement devenus des enjeux strat√©giques et √©conomiques pour les entreprises. Aujourd'hui, rares sont les domaines qui ont √©chapp√© √† leur influence.\n",
        "\n",
        "IA g√©n√©rative, ChatGPT, LLM... le lexique associ√© √† ces technologies est dense et opaque. Le tableau suivant vise √† clarifier ces concepts en les comparant aux composants d'un v√©hicule, afin de mieux comprendre leur r√¥le et leur fonctionnement au sein d'une application d'IA g√©n√©rative.\n",
        "\n",
        "**Composants d'une application d'IA g√©n√©rative :**\n",
        "\n",
        "| Composant | Analogie v√©hicule | R√¥le | Exemples |\n",
        "|--|--|--|--|\n",
        "| LLM | Moteur | Le c≈ìur du syst√®me, c‚Äôest ce qui fait tourner l'application | GPT-4o / Mistral Large / Claude 3.7 Sonnet / DeepSeek R1 |\n",
        "| Interface utilisateur (UI) | Carrosserie | Ce que l‚Äôutilisateur voit et utilise pour interagir avec le LLM. | ChatGPT / Le Chat / Perplexity |\n",
        "| Outils | Options | Permettent d‚Äôenrichir les capacit√©s de base | Recherche web / Ex√©cution de code / Analyse de documents |\n",
        "\n",
        "L'objectif de ce TP est d'explorer et de comprendre le fonctionnement des moteurs d'applications d'IA g√©n√©rative, les **LLMs**, afin de mieux appr√©hender leurs enjeux et leurs limites.\n",
        "\n",
        "**Plan du TP :**\n",
        "1. Choix et chargement du LLM\n",
        "2. Exploration du fonctionnement du LLM\n",
        "3. Comment transformer un LLM en un assistant ?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2SGVlxK02ms"
      },
      "source": [
        "## **0. Installations, imports et d√©finition de fonctions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyERiSMGLISk"
      },
      "source": [
        "### Imports et param√®tres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dhSJAuai4Ly"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer, logging\n",
        "\n",
        "\n",
        "\n",
        "# Set device and corresponding data type\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DTYPE = torch.float16 if DEVICE==\"cuda\" else torch.float32\n",
        "\n",
        "# Disable torch gradient tracking\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Hide warnings\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cl√©s d'API"
      ],
      "metadata": {
        "id": "z_-Sf9AdjCIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hugging Face token\n",
        "os.environ[\"HF_TOKEN\"] = \"\" # ask for your token !"
      ],
      "metadata": {
        "id": "-rjjoM9qjFvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95q9EjVSLISk"
      },
      "source": [
        "### D√©finition des fonctions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_token_embeddings(token_ids: list[int]) -> None:\n",
        "    \"\"\"\n",
        "    Plots 2D representations of token embeddings for a given list of token IDs.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the Hugging Face model to load.\n",
        "        token_ids (list): List of token IDs for which embeddings are extracted.\n",
        "    \"\"\"\n",
        "    # Extract embeddings\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.get_input_embeddings()(torch.tensor(token_ids).to(DEVICE))\n",
        "\n",
        "    # Reduce dimensions to 2D using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_2d = pca.fit_transform(embeddings.cpu().detach().float().numpy())\n",
        "\n",
        "    # Plot the embeddings with vectors\n",
        "    plt.figure()\n",
        "    plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)  # Add horizontal line at y=0\n",
        "    plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)  # Add vertical line at x=0\n",
        "\n",
        "    for i, token_id in enumerate(token_ids):\n",
        "        x, y = embeddings_2d[i]\n",
        "        plt.quiver(0, 0, x, y,\n",
        "                  angles='xy', scale_units='xy', scale=1,\n",
        "                  color='blue', alpha=0.5)\n",
        "        plt.scatter(x, y, color='red', zorder=3)\n",
        "        plt.text(x, y, tokenizer.decode([token_id]),\n",
        "                fontsize=12, ha='left', va='bottom')\n",
        "\n",
        "    plt.title(f\"2D Token Embeddings for {model_name}\")\n",
        "    plt.xlabel(\"PCA Dimension 1\")\n",
        "    plt.ylabel(\"PCA Dimension 2\")\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0ax2m_n0VY_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAAgU1mQ0PGZ"
      },
      "source": [
        "## **1. Choix et chargement du LLM**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En pratique, un LLM se compose de deux fichiers :\n",
        "- Un fichier de code permettant d'ex√©cuter le mod√®le, compos√©s de quelques centaines de lignes (~1 Mo)\n",
        "- Un fichier contenant les param√®tres (poids) du mod√®le. Les plus petits mod√®les sont compos√©s de quelques milliards de param√®tres (1-10 Go) et les plus grands peuvent en poss√©der plusieurs centaines de milliards (+1 To)\n",
        "\n",
        "Voici, pour quelques mod√®les populaires, le nombre de param√®tres ainsi que la taille du fichier associ√© :\n",
        "\n",
        "| **LLM**               | **Nombre de param√®tres** | **Taille du fichier**  |\n",
        "|--------------------------|--------------------------|------------------------|\n",
        "| Mistral-7B               | 7 milliards              | 14 Go                  |\n",
        "| Meta-Llama-3-70B         | 70 milliards             | 140 Go                 |\n",
        "| GPT-3.5 (ChatGPT 2022)   | 175 milliards            | 350 Go                 |\n",
        "| DeepSeek-R1              | 685 milliards            | 1370 Go                |\n",
        "\n",
        "Certaines entreprises publient leurs LLMs en open-source (Meta, Mistral, DeepSeek...), ce qui permet de les t√©l√©charger et de les ex√©cuter sur un PC pour les plus petits ou un serveur pour les plus gros. Les autres (OpenAI, Google, Claude...) ne donnent pas acc√®s √† leurs mod√®les rendant toute utilisation *on-premise* impossible."
      ],
      "metadata": {
        "id": "XZILxikV28J8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 D√©couverte d'Hugging Face ü§ó**"
      ],
      "metadata": {
        "id": "ngavCqmLFCoQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZVXWkZjSPHn"
      },
      "source": [
        "[Hugging Face ü§ó](https://huggingface.co/) est une plate-forme permettant le partage de mod√®les, de code et de donn√©es dans le domaine du **NLP (Natural Language Processing)**, et plus marginalement dans d'autres domaines de l'intelligence artificielle (vision par ordinateur, apprentissage par renforcement). Le site permet √† n'importe qui (du simple utilisateur aux grandes entreprises comme **Meta** ou **Mistral**) de partager son travail pour qu'il soit utilisable par la communaut√©.\n",
        "\n",
        "Pour explorer les mod√®les disponibles, rendez-vous dans l'onglet [models](https://huggingface.co/models).\n",
        "\n",
        "Pour ce TP, nous allons utiliser un \"petit\" LLM : [Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B), d√©velopp√© par **Meta**.\n",
        "\n",
        "Il est compos√© de 1,24 milliards de param√®tres et p√®se 2,47 Go.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Chargement du mod√®le et de son tokenizer**"
      ],
      "metadata": {
        "id": "ZcM0-PuxHobg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous utilisons la librairie python [transformers](https://github.com/huggingface/transformers) d√©velopp√©e par Hugging Face qui permet de charger et d'utiliser facilement les mod√®les disponibles sur la plateforme."
      ],
      "metadata": {
        "id": "--Dh2k9Y9Ysx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B\""
      ],
      "metadata": {
        "id": "RVqIj_mCIMM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "rg9TMIJWI7zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=DEVICE,\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Kv_hBHI8I0sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put model into inference mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "gdwLgq94JAKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Exploration du fonctionnement du LLM**"
      ],
      "metadata": {
        "id": "lw2W_VpYZw-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les LLMs font partie d'une cat√©gorie d'algorithmes nomm√©s **mod√®les de langage** et dont l'objectif fondamental et de pr√©dire le mot suivant √† partir d'une s√©quence de mots.\n",
        "\n",
        "Dans cette section, nous allons explorer comment un LLM fonctionne pour pr√©dire le mot suivant. Dans la section suivante, nous expliquerons comment, √† partir d'un mod√®le qui pr√©dit le mot suivant, il est possible de simuler un assistant type ChatGPT.\n",
        "\n",
        "Le fonctionnement du LLM peut √™tre divis√© en 3 √©tapes :\n",
        "- tokenizer : d√©coupe le texte brut en une s√©quence de token lisible par le LLM\n",
        "- embedding : transforme chaque token en un vecteur num√©rique qui capture la s√©mantique du token\n",
        "- r√©seau de neurones : g√©n√®re le token suivant\n",
        "\n"
      ],
      "metadata": {
        "id": "f6uca7o1-Uq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Tokenizer**"
      ],
      "metadata": {
        "id": "p3wYfiyzJOmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le tokenizer est un algorithme s√©par√© du LLM. Chaque LLM poss√®de son propose tokenizer. Son objectif est de transformer le texte brut en une s√©rie de tokens que le LLM peut comprendre et traiter. Le tokenizer poss√®de un nombre fini de tokens que l'on appelle le **vocabulaire** du LLM."
      ],
      "metadata": {
        "id": "nLye3voiAsUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show LLM vocabulary size\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
      ],
      "metadata": {
        "id": "cgH8ubsbEvdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt\n",
        "prompt = \"RTE est le gestionnaire du r√©seau de transport d'√©lectricit√©.\"\n",
        "\n",
        "# Try tokenizer functions\n",
        "tokenized = tokenizer.tokenize(prompt)\n",
        "encoded = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "\n",
        "# Check differences between each function\n",
        "print(f\"Tokenized : {[tokenizer.decode(token) for token in encoded]}\")\n",
        "print(f\"Encoded : {encoded}\")"
      ],
      "metadata": {
        "id": "x6QD2ApFJS_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Question**\n",
        "\n",
        "Rendez-vous sur [tiktokenizer/Meta-Llama-3-8B](https://tiktokenizer.vercel.app/?model=meta-llama%2FMeta-Llama-3-8B) pour essayer le tokenizer de mani√®re plus interactive.\n",
        "\n",
        "Essayez √©galement le tokenizer d'un LLM √† l'√©tat de l'art, par exemple celui du mod√®le `GPT-4o` d'OpenAI :  [tiktokenizer/o200k_base](https://tiktokenizer.vercel.app/?model=o200k_base).\n",
        "\n",
        "Que remarquez-vous ? Quelles cons√©quences ?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XF6IUXTKw-Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Embedding**"
      ],
      "metadata": {
        "id": "e4m5S-2iNFHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les ordinateurs ne pouvant effectuer des op√©rations que sur des nombres, il est n√©cessaire de convertir chaque token en chiffres. Dans les LLMs, cette conversion se fait √† l'aide des **embeddings**, des vecteurs num√©riques qui repr√©sentent la s√©mantique du token : deux tokens qui ont un sens proche sont repr√©sent√©s par des vecteurs num√©riques similiares.\n",
        "\n",
        "La premi√®re couche du LLM est une simple table de correspondance qui associe √† chaque token un embedding."
      ],
      "metadata": {
        "id": "As-OOY95FAaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print LLM embedding layer shape\n",
        "model.get_input_embeddings()"
      ],
      "metadata": {
        "id": "cuda4UrQxvp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of waords\n",
        "word_list = \"cat dog green blue house room\"\n",
        "\n",
        "# Plot tokens embeddings in a reduce 2D space\n",
        "prompt_tokens_ids = list(set(tokenizer.encode(word_list, add_special_tokens=False)))\n",
        "plot_token_embeddings(prompt_tokens_ids)"
      ],
      "metadata": {
        "id": "zeRxbslANodc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Question**\n",
        "\n",
        "Ajoutez des mots √† la liste et observez le comportement.\n",
        "\n",
        "Quelles limites anticipez-vous avec cette mani√®re de repr√©senter les mots ? Quel va √™tre le r√¥le du r√©seau de neurones qui vient apr√®s ?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TJNH2MSsGG5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 R√©seau de neurones**"
      ],
      "metadata": {
        "id": "ZNjLacH1Q8vA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'essentiel des param√®tres du LLM sont contenus dans le r√©seau de neurones.\n",
        "\n",
        "Ce r√©seau de neurone peut-√™tre vu comme un simple classifieur. A partir d'une liste de tokens en entr√©e, il retourne en sortie la probabilit√© pour chaque token de son vocabulaire d'√™tre le suivant.\n",
        "\n"
      ],
      "metadata": {
        "id": "8gJxEzhlGrMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt and temperature\n",
        "prompt = \"The capital of Australia is\"\n",
        "\n",
        "# Tokenize prompt\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Make a forward pass\n",
        "output = model.forward(input_ids=inputs, use_cache=False)['logits'].squeeze()[-1]\n",
        "\n",
        "# Show results shape\n",
        "print(f\"Output: {output}\")\n",
        "print(f\"Output length: {len(output)}\")"
      ],
      "metadata": {
        "id": "XQMWd-P1RFeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define temperature\n",
        "temperature = 0.5\n",
        "\n",
        "# Transform model output to probability distribution using softmax\n",
        "output_softmax = torch.nn.functional.softmax(output/temperature, dim=0)\n",
        "\n",
        "# Filter top_k tokens\n",
        "top_k = 10\n",
        "top_k_indices = torch.topk(output_softmax, top_k).indices\n",
        "top_k_probabilities = output_softmax[top_k_indices].cpu().detach().float().numpy()\n",
        "top_k_tokens = [tokenizer.decode([token_id]) for token_id in top_k_indices]\n",
        "\n",
        "# Barplot\n",
        "px.bar(x=top_k_tokens, y=top_k_probabilities, labels={\"x\": \"token\", \"y\": \"probability\"}, title=prompt+\"...\")"
      ],
      "metadata": {
        "id": "1Vl2rQuMScSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Question**\n",
        "\n",
        "Un param√®tre important est le param√®tre `temperature` lors de l'op√©ration softmax.\n",
        "\n",
        "Modifiez ce param√®tre entre 0 et 1 et observez la distribution de sortie.\n",
        "\n",
        "Quelle influence ce param√®tre aura-t-il sur le texte g√©n√©r√© par le LLM ?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nStvt_aDI_LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4 Pipeline de g√©n√©ration**"
      ],
      "metadata": {
        "id": "fxD9iguAaBd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour g√©n√©rer du texte, il suffit de r√©p√©ter le processus suivant :\n",
        "- faire une pr√©diction avec le mod√®le\n",
        "- √©chantillonner la sortie pour s√©lectionner un token\n",
        "- ajouter ce token √† la s√©quence de tokens d'entr√©e"
      ],
      "metadata": {
        "id": "E8zx0dmYKV9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamer to print token on the fly\n",
        "streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
        "\n",
        "# Generation pipeline to chain model predictions\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, streamer=streamer)"
      ],
      "metadata": {
        "id": "_EPbszkbTHnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt\n",
        "prompt = \"The capital of Australia is\"\n",
        "\n",
        "# Generate text continuation\n",
        "prediction = generator(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.5,\n",
        "    max_new_tokens=30\n",
        ")"
      ],
      "metadata": {
        "id": "64YjhlB7a8rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Question**\n",
        "\n",
        "Modifiez le param√®tre temp√©rature et observez le texte g√©n√©r√©.\n",
        "\n",
        "Dans quels cas utiliser une temp√©rature √©lev√©e ? Dans quels cas utiliser une temp√©rature faible ?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-Lq0P6tyK6lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.5 Limites de taille m√©moire**"
      ],
      "metadata": {
        "id": "uhflj_FdHp8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La taille du prompt impact directement la quantit√© de m√©moire n√©cessaire pour g√©n√©rer des tokens.\n",
        "\n",
        "*La cellule suivante ne fonctionne que si le LLM est charg√© sur GPU.*"
      ],
      "metadata": {
        "id": "1bTW-2-zH4vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_memory(token_count: int) -> int:\n",
        "    \"\"\"\n",
        "    Returns total GPU memory used for processing `token_count` tokens.\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    dummy_token_id = tokenizer.encode(\"the\")[0]\n",
        "    input_ids = torch.tensor([[dummy_token_id] * token_count], device=\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids)\n",
        "\n",
        "    return torch.cuda.max_memory_allocated() / 1024**2\n",
        "\n",
        "# Set list of tokens to process\n",
        "token_counts = [2**i for i in range(0, 14)]\n",
        "baseline = measure_memory(1)\n",
        "\n",
        "# Compute overheads\n",
        "overheads = []\n",
        "for n in token_counts:\n",
        "    mem = measure_memory(n)\n",
        "    overheads.append(mem - baseline)\n",
        "    print(f\"{n} tokens ‚Üí overhead: {mem - baseline:.2f} MB\")\n",
        "\n",
        "# Plot graph\n",
        "plt.plot(token_counts, overheads, marker=\"o\")\n",
        "plt.title(\"Memory Overhead vs Token Count\")\n",
        "plt.xlabel(\"Number of Tokens\")\n",
        "plt.ylabel(\"Overhead Memory (MB)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ms85n4Z1JVVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Comment transformer un LLM en un assistant ?**"
      ],
      "metadata": {
        "id": "1snT6tOydTll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un LLM n'est qu'un moteur √† g√©n√©rer du texte. Ce n'est pas encore un assistant qui r√©pond √† nos questions ! Dans l'exemple suivant, observez comme le LLM continue notre s√©quence de tokens par un texte probable (un message sur un blog) sans r√©pondre √† la question."
      ],
      "metadata": {
        "id": "c68btU3e7Ekj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt\n",
        "prompt = \"How to remove an item from a list in python?\"\n",
        "\n",
        "# Generate text continuation\n",
        "prediction = generator(\n",
        "    prompt,\n",
        "    do_sample=False,\n",
        "    temperature=None,\n",
        "    top_p=None,\n",
        "    max_new_tokens=30\n",
        ")"
      ],
      "metadata": {
        "id": "EX9sVOnqTIhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour transformer un LLM en assistant, il faut utiliser deux astuces qui vont permettre de mieux contr√¥ler le texte g√©n√©r√©."
      ],
      "metadata": {
        "id": "ZnBkY9axLnU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Chat prompt template"
      ],
      "metadata": {
        "id": "HyxI7XDb86zO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous voulons int√©ragir avec le LLM sous la forme d'une conversation dans laquelle il r√©pond aux questions que l'on pose. L'astuce consiste √† modifier la structure de notre prompt pour faire comprendre au LLM qu'il se trouve dans une conversation et qu'il joue le r√¥le de l'assistant."
      ],
      "metadata": {
        "id": "MzJJX1VQ9MAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "USER:\n",
        "    In python, how to compute the number of items in a list?\n",
        "ASSISTANT:\n",
        "    You can use the following function: `len(my_list)`.\n",
        "USER:\n",
        "    How to remove an item from a list in python?\n",
        "ASSISTANT:\n",
        "\"\"\"\n",
        "\n",
        "# Generate text continuation\n",
        "output = generator(\n",
        "    prompt,\n",
        "    do_sample=False,\n",
        "    temperature=None,\n",
        "    top_p=None,\n",
        "    max_new_tokens=40\n",
        ")"
      ],
      "metadata": {
        "id": "-h1VdvKbUwXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le LLM comprend qu'apr√®s le token `ASSISTANT` se trouve une r√©ponse √† la question pos√©e apr√®s le token `USER`."
      ],
      "metadata": {
        "id": "UC1s6AsGMyf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Arr√™ter la g√©n√©ration √† la fin de la r√©ponse"
      ],
      "metadata": {
        "id": "ItaaFKND-qhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour que le LLM s'arr√™te √† la fin de la r√©ponse `ASSISTANT`, on peut ajouter un token `FIN` pour signifier la fin d'un message puis arr√™ter la g√©n√©ration lorsque ce token est g√©n√©r√©."
      ],
      "metadata": {
        "id": "SlO3cfVC-tiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "USER:\n",
        "    In python, how to compute the number of items in a list?\n",
        "FIN\n",
        "ASSISTANT:\n",
        "    You can use the following function: `len(my_list)`.\n",
        "FIN\n",
        "USER:\n",
        "    How to remove an item from a list in python?\n",
        "FIN\n",
        "ASSISTANT:\n",
        "\"\"\"\n",
        "\n",
        "# Generate text continuation\n",
        "output = generator(\n",
        "    prompt,\n",
        "    do_sample=False,\n",
        "    temperature=None,\n",
        "    top_p=None,\n",
        "    max_new_tokens=30,\n",
        "    tokenizer=tokenizer,\n",
        "    stop_strings=\"FIN\" # stop generation when this token is sampled\n",
        ")"
      ],
      "metadata": {
        "id": "9E1X7lIDU11l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Question**\n",
        "\n",
        "Selon vous, comment est g√©r√©e la m√©moire de la conversation ?\n",
        "\n",
        "Qu'est-ce que cela implique en terme de temps de calcul / co√ªts ?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Ix_dzrwADym"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZyERiSMGLISk",
        "z_-Sf9AdjCIT",
        "95q9EjVSLISk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}