{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook facultatif : préparation du jeu de données à partir des données brutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand on se lance avec enthousiasme dans un nouveau projet de machine-learning, on pense avant tout au choix du modèle que l'on va utiliser, ce qui constitue la partie \"noble\" de l'étude. \n",
    "\n",
    "Cependant, et même s'il ne s'agit pas de l'étape la plus agréable du travail, un préalable indispensable est de réunir les données brutes puis de les mettre en forme pour qu'elles puissent être ingérées par le modèle.\n",
    "\n",
    "L'objectif de ce TP est d'effectuer ce travail de construction du jeux de données \"propres\" à partir de données brutes issues de sources diverses.\n",
    "\n",
    "Nos fichiers d'entrée bruts sont les suivants :\n",
    "> * historique de consommation nationale (source : Eco2mix) : **conso_2014_2018.csv**\n",
    "> * calendrier des jours fériés : **jours_feries.csv**\n",
    "> * coordonnées géographiques et les poids associés aux stations météo France : **stations_meteo_RTE.csv**\n",
    "> * historique d'observations et de prévisions à 24h de température : **temperatures.csv**\n",
    "\n",
    "Et les fichiers que l'on va créer sont :\n",
    "> * **x_input_full.csv**  # Les entrées pour le modèle d'apprentissage\n",
    "> * **x_input_light.csv**  # Une version allégée de *x_input_full*\n",
    "> * **y_conso.csv**  # les sorties pour le modèle d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place de l'environnement\n",
    "\n",
    "Exécutez la cellule ci-dessous, par exemple avec shift-entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des bibliothèques python, et quelques éléments de configuration. \n",
    "# Si vous exécuter ce notebook depuis votre PC, il faudra peut-etre installer certaines librairies avec   \n",
    "# \"pip3 install ma_bibliotheque\"\n",
    "import os  # accès aux commandes système\n",
    "import datetime  # structure de données pour gérer des objets calendaires\n",
    "import pandas as pd  # gérer des tables de données en python\n",
    "import numpy as np  # librairie d'opérations mathématiques\n",
    "import zipfile  # manipulation de fichiers zip\n",
    "from sklearn.preprocessing import StandardScaler  # normalisation des données\n",
    "import joblib  # sauvegarde du scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "raw_data_folder = os.path.join(data_folder, \"raw_data\")\n",
    "transformed_data_folder = os.path.join(data_folder, \"transformed_data\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Mon repertoire de raw_data_folder est : {}\".format(raw_data_folder))\n",
    "print(\"\")\n",
    "print(\"Fichiers/dossiers contenus dans ce repertoire :\")\n",
    "for file in os.listdir(raw_data_folder):\n",
    "    print(\" - \" + file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import en mémoire des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données de consommation\n",
    "\n",
    "Dans un premier temps on importe les données de consommation réalisée à partir du fichier **conso_2014_2018.csv**.   \n",
    "La date et l'heure sont données dans la première colonne, la consommation nationale dans la suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données du csv sont importées dans un objet de type DataFrame\n",
    "conso_csv = os.path.join(raw_data_folder, \"conso_2014_2018.csv\")\n",
    "conso_df = pd.read_csv(conso_csv, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions que les données ont été importées correctement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les dimensions et le noms des colonnes de la data frame\n",
    "print(conso_df.shape)  # Nombre de lignes, nombre de colonnes\n",
    "display(conso_df.head(2))\n",
    "display(conso_df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Petit détour pour gérer les dates\n",
    "\n",
    "Le fichier **conso_2014_2018.csv** contient en particulier la colonnes **ds**. Celle-ci contient une variable de type *string* correspondant à la date et à l'heure. \n",
    "\n",
    "<img src=\"pictures/clock.png\" width=60 height=60>\n",
    "\n",
    "Pour manipuler facilement ces variables, il est recommandé de les convertir en objet de type **datetime**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conso_df['ds'] = pd.to_datetime(conso_df['ds'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(conso_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule ci-dessous a pour but d'illustrer comment utiliser ces objets **datetime**.  \n",
    "Ces quelques lignes de code pourraient vous reservir ultérieurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime vers string\n",
    "noel_2017_date = datetime.date(2017, 12, 25)\n",
    "noel_2017_str = datetime.datetime.strftime(noel_2017_date, format=\"%Y-%m-%d\")\n",
    "print(\"noel_2017_date vaut : {}, et est de type {}\".format(noel_2017_date, str(type(noel_2017_date))))\n",
    "print(\"noel_2017_str vaut : {}, et est de type {}\".format(noel_2017_str, str(type(noel_2017_str))))\n",
    "print(\"---\")\n",
    "\n",
    "# string vers datetime\n",
    "starwars_day_2017_str = \"2017-05-04\"\n",
    "starwars_day_2017_date = datetime.datetime.strptime(starwars_day_2017_str, \"%Y-%m-%d\")\n",
    "print(\"starwars_day_2017_date vaut : {}, et est de type {}\".format(starwars_day_2017_date, str(type(starwars_day_2017_date))))\n",
    "print(\"D'ailleurs, c'était le \" + str(starwars_day_2017_date.weekday() + 1) + \" ème jour de la semaine, où 0 correspond à lundi et 6 correspond à dimanche\")\n",
    "print(\"starwars_day_2017_str vaut : {}, et est de type {}\".format(starwars_day_2017_str, str(type(starwars_day_2017_str))))\n",
    "print(\"---\")\n",
    "\n",
    "# Voyager dans le temps\n",
    "saint_sylvestre_2017_date = datetime.date(2017, 12, 31)\n",
    "bienvenu_en_2018_date = saint_sylvestre_2017_date + datetime.timedelta(days=1)\n",
    "print(\"Le 31 décembre 2017 plus un jour ça donne le {}\".format(bienvenu_en_2018_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récuperation des jours fériés\n",
    "\n",
    "Le fichier **jours_feries.csv** contient la liste des jours fériés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jours_feries_csv = os.path.join(raw_data_folder,\"jours_feries.csv\")\n",
    "jours_feries_df = pd.read_csv(jours_feries_csv, sep=\";\")\n",
    "\n",
    "jours_feries_df.ds = pd.to_datetime(jours_feries_df.ds, utc=True)  # comme ci-dessus\n",
    "\n",
    "jours_feries_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des coordonnées géographiques des stations météo\n",
    "\n",
    "Dans le but d'obtenir un modèle de machine-learning performant, nous allons créer une variable artificielle qui sera représentative de la température nationale France. Cette variable n'aura été relevée par aucun thermomètre, mais sera un barycentre des températures mesurées aux différentes stations météo. Chaque station météo s'est vue attribuer un poids permettant de sentir sa contribution. Pour en savoir plus sur les poids :  \n",
    "https://clients.rte-france.com/lang/fr/visiteurs/services/actualites.jsp?id=9482&mode=detail\n",
    "\n",
    "Commençons par charger le csv qui à chaque station météo attribue sa longitude/latitude/poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_meteo_csv = os.path.join(raw_data_folder, \"stations_meteo_RTE.csv\")\n",
    "stations_meteo_df = pd.read_csv(stations_meteo_csv, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_meteo_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour compter le nombre de stations il suffit de compter le nombre de lignes dans le data-frame\n",
    "nb_stations = stations_meteo_df.shape[0]\n",
    "print(nb_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifions que la somme des poids fait 1\n",
    "np.sum(stations_meteo_df[\"Poids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des températures\n",
    "\n",
    "<img src=\"pictures/weather.png\" width=60 height=60>\n",
    "\n",
    "On va utiliser les mêmes fonctions que précédemment pour lire le fichier **temperatures.csv**, qui contient les historiques des température réalisées et prévues pour différentes stations météo France.\n",
    "\n",
    "**Attention : Les données météo sont encryptées dans un fichier zip.**  \n",
    "Pour les lire vous avez besoin d'un mot de passe qui ne peut vous être donné que dans le cadre d'un travail au sein de RTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_zip = os.path.join(raw_data_folder, \"temperatures.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "password = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette étape peut être un peu longue car le fichier est volumineux\n",
    "zip_file_meteo = zipfile.ZipFile(meteo_zip)\n",
    "zip_file_meteo.setpassword(bytes(password, 'utf-8'))\n",
    "meteo_df = pd.read_csv(zip_file_meteo.open('temperatures.csv'), sep=\",\", engine='c', header=0)\n",
    "meteo_df['ds'] = pd.to_datetime(meteo_df.date_cible, utc=True)\n",
    "meteo_df = meteo_df.drop(columns=['date_cible'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meteo_df.shape)  # (nb lignes , nb_colonnes)\n",
    "print(meteo_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(meteo_df.head(3))\n",
    "display(meteo_df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les données s'arrête au 21/12/2018 et que pour cette date, on ne dispose que des prévisions à 24h, pas des réalisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Une variable complémentaire à considérer : les données TEMPO\n",
    "\n",
    "Pour l'exemple de la prévision de consommation, il serait pertinent de fournir en entrée du modèle l'information sur le type de jour Tempo. Les clients ayant souscrit à ce type de contrat sont incités à réduire leur consommations les jours BLANC et ROUGE, aussi on peut penser que cette information permettra d'améliorer la qualité des prédictions.\n",
    "\n",
    "Elles ne sont pas utilisées dans le cadre de ce TP, mais le stagiaire motivé pourra trouver des informations complémentaires sur le web :  \n",
    "http://www.rte-france.com/fr/eco2mix/eco2mix-telechargement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fusion des données\n",
    "\n",
    "<img src=\"pictures/fusion.png\" width=600 height=200>\n",
    "\n",
    "On va maintenant construire un dataframe unique qui regroupe toutes les données nécessaire à notre modèle de prévision. On aura ici une ligne pour chaque timestamp, et dans cette ligne à la fois notre X et notre Y pour le futur modèle de machine-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion des données de consommation et de jours fériés\n",
    "\n",
    "**Attention**  \n",
    "L'heure n'apparaît pas dans *jours_feries_df*, mais implicitement, elle vaut 00:00 pour toutes les observations.  \n",
    "Il faut y prendre garde lors de la la fusion avec les données de consommation, afin que le point horaire de 15h35 d'un jour férié soit bien reconnu comme appartenant à un jour férié. Pour cela, nous allons créer dans les deux jeux de données une colonne **day**, qui ne conserve que la date. Nous utiliserons ensuite cette colonne pour fusionner les deux jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On rajoute la colonne *day* aux 2 dataframes\n",
    "conso_df['day'] = conso_df['ds'].apply(lambda x: datetime.datetime.strftime(x, format=\"%Y-%m-%d\"))\n",
    "jours_feries_df['day'] = jours_feries_df['ds'].apply(lambda x: datetime.datetime.strftime(x, format=\"%Y-%m-%d\"))\n",
    "\n",
    "# On merge sur cette colonne\n",
    "merged_df = pd.merge(conso_df, jours_feries_df, on='day', how=\"left\", suffixes=(\"\", \"_tmp\"))\n",
    "\n",
    "# On peut maintenant supprimer ces colonnes temporaires\n",
    "merged_df = merged_df[[\"ds\", \"conso_real\", \"holiday\"]]\n",
    "display(merged_df.loc[0:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"is_bank_holiday\"] = merged_df[\"holiday\"].astype(\"str\").apply(lambda x: 0 if x == \"nan\" else 1)\n",
    "merged_df.drop(['holiday'], axis=1, inplace=True)\n",
    "\n",
    "# Regardons\n",
    "display(merged_df.loc[0:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout des informations de température."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, meteo_df, on = 'ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.shape)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de la variable artificielle \"température France\"\n",
    "\n",
    "On va ajouter une colonne à notre dataframe, colonne que - par expérience/expertise - on sait pouvoir être utile pour prévoir la consommation.\n",
    "\n",
    "La température France est une moyenne pondérée de la température de 32 stations. C'est ici que nous avons besoin des poids de *stations_meteo_df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "merged_df['temperature_real'] = np.dot(merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"_0\")])], stations_meteo_df['Poids'])\n",
    "merged_df['temperature_prevue'] = np.dot(merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"_24\")])], stations_meteo_df['Poids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.shape)\n",
    "print(merged_df.columns)\n",
    "display(merged_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout des variables retardées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle prédictif n'aura pas le droit d'avoir accès à la température réalisée du futur (dommage, ça rendrait notre métier plus facile :-)), mais en revanche nous avons accès à la connaissance de la température réalisée 24 heures avant.\n",
    "\n",
    "De même pour la consommation.\n",
    "\n",
    "Ainsi, nous avons accès au réalisé (température, conso nationale) du J-1, et nous avons une température prévue pour le point horaire cible. Charge à notre modéle de prévision de nous proposer une valeur d ela consommation nationale pour le point horaire cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"temperature_real_24h_avant\"] = merged_df[\"temperature_real\"].shift(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"conso_real_24h_avant\"] = merged_df[\"conso_real\"].shift(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**Prenons quelques instants pour comprendre ces histoires de variables retardées.**\n",
    "\n",
    "Imaginons que l'on veuille prédire la consommation nationale pour le point horaire cible du 09/01/2014 à 00h00 (ligne 24 de working_df)\n",
    "\n",
    "Utiliser dans un modèle de prédiction :\n",
    "* la consommation réalisée de la veille (08/01/2014 à 00h00)\n",
    "* les infos de météo réalisée de la veille\n",
    "* les **prévisions** météo du point horaire cible (09/01/2014 à 00h00)\n",
    "\n",
    "semble être pertinent.\n",
    "\n",
    "</font>\n",
    "<br/>\n",
    "  \n",
    "<font color='green'>\n",
    "\n",
    "* Est-ce que le shift a été fait dans le bon sens ?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supression des variables inutiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(\"temperature_real\", axis=1)\n",
    "merged_df = merged_df.drop([x for x in merged_df.columns if (x.endswith(\"_0\") or x.endswith(\"_24\"))], axis=1)\n",
    "\n",
    "print(merged_df.shape)\n",
    "print(merged_df.columns)\n",
    "display(merged_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression des NaN\n",
    "\n",
    "On souhaite avoir un jeu de données complet : pas de trous dans les dates et pour chaque point horaire, toutes les informations disponibles (consommation, température réalisée et prévue).\n",
    "\n",
    "On affiche donc les dates pour lesquelles il manque des informations et on conservera un intervalle ne contenant pas ces dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = merged_df.isnull().any(axis=1)\n",
    "\n",
    "print(np.sum(mask))\n",
    "print(merged_df.index[mask].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En début de dataset, avec nos manipulation pour obtenir les valeurs de la veille, il est normal que l'on ait quelques *NaN* pour les colonnes 24h_avant.\n",
    "\n",
    "Quant à la ligne d'index 165, bah... C'est la vie.\n",
    "\n",
    "maintenant nous n'allons garder que les lignes sans *NaN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[~mask]\n",
    "merged_df.reset_index(drop=True, inplace=True)  # on demande de remettre à jour les numéros d'index de la DF pour commencer à 0\n",
    "\n",
    "print(merged_df.shape)\n",
    "print(merged_df.columns)\n",
    "display(merged_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation (\"scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = [\"conso_real\", \"temperature_prevue\", \"temperature_real_24h_avant\", \"conso_real_24h_avant\"]\n",
    "new_column_names = [x + \"_scaled\" for x in columns_to_scale]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(merged_df[columns_to_scale])\n",
    "scaled_values = scaler.transform(merged_df[columns_to_scale])\n",
    "scaled_values_df = pd.DataFrame(scaled_values, columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remarque en passant:\n",
    "# Ne pas relancer cette cellule, sinon\n",
    "# working_df va beaucoup grossir...\n",
    "merged_df = pd.concat([merged_df, scaled_values_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merged_df.head(2))\n",
    "print(merged_df.shape)\n",
    "\n",
    "# Petite verif manuelle\n",
    "mean = np.mean(merged_df[\"conso_real\"])\n",
    "mean_scaled = np.mean(merged_df[\"conso_real_scaled\"])\n",
    "print()\n",
    "print(\"--- Petite verif ---\")\n",
    "print(\"Moyenne de conso_real: \" + str(mean))\n",
    "print(\"Moyenne de conso_real_scaled: \" + str(mean_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour plus tard, on se garde de côté un scaler uniquement dénormaliser la prédiction de conso\n",
    "scaler_conso_nat = StandardScaler()\n",
    "scaler_conso_nat.fit(merged_df[[\"conso_real\"]])\n",
    "\n",
    "# Verif\n",
    "scaler_conso_nat.inverse_transform(merged_df[\"conso_real_scaled\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding de la date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['month'] = merged_df['ds'].dt.month\n",
    "merged_df['hour'] = merged_df['ds'].dt.hour\n",
    "\n",
    "# On sépare les jours de la semaine en week-end / pas week-end\n",
    "# De base, la fonction datetime.weekday() renvoie 0 => Lundi, 2 => Mardi, ..., 5 => Samedi, 6 => Dimanche\n",
    "# Ci-dessous, si on a un jour de la semaine alors dans la colonne weekday on mettra 1, et 0 si c'est le week-end\n",
    "merged_df['weekday'] = (merged_df['ds'].dt.weekday < 5).astype(int)  # conversion bool => int\n",
    "\n",
    "# Regardons\n",
    "merged_df.loc[[0, 1, 23, 24, 24 *7 -1, 24 * 7, 1100], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant effectuer un **\"one-hot encoding\"** pour convertir ces informations sous un format utilisable par un réseau de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_month = pd.get_dummies(merged_df['month'], prefix=\"month\")\n",
    "encoded_hour = pd.get_dummies(merged_df['hour'], prefix=\"hour\")\n",
    "\n",
    "display(encoded_month.head(3))\n",
    "display(encoded_hour.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne calculer cette cellule qu'une seule fois à cause du concat !\n",
    "merged_df = pd.concat([merged_df, encoded_month, encoded_hour], axis=1)\n",
    "merged_df = merged_df.drop(['month', 'hour'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.shape)\n",
    "print(merged_df.columns)\n",
    "display(merged_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du fichier \n",
    "\n",
    "Tout d'abord on sépare les données en deux : \n",
    "- le vecteur de consommation à prévoir : y_conso\n",
    "- La matrice des variables explicatives : x_input\n",
    "\n",
    "Sachant que plus tard notre modèle aura pour mission d'établir une correspondance _f_ telle que l'on ait du mieux possible une relation *y = f(X)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = [\"ds\", \"conso_real\", \"conso_real_scaled\"]\n",
    "\n",
    "x_columns = [\"ds\", \"is_bank_holiday\"] \\\n",
    "+ [\"temperature_real_24h_avant\", 'temperature_real_24h_avant_scaled'] \\\n",
    "+ [\"temperature_prevue\", 'temperature_prevue_scaled'] \\\n",
    "+ [\"conso_real_24h_avant\", 'conso_real_24h_avant_scaled'] \\\n",
    "+ [\"month_\" + str(x) for x in range(1, 13)] \\\n",
    "+ [\"hour_\" + str(x) for x in range(0, 24)] \\\n",
    "+ [\"weekday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = merged_df[y_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = merged_df[x_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv(os.path.join(transformed_data_folder,\"x.csv\"), index=False, sep=\";\")\n",
    "y.to_csv(os.path.join(transformed_data_folder,\"y.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler_conso_nat, os.path.join(transformed_data_folder, \"scaler_conso.save\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et enfin on zip **x.csv** avec un mot de passe car ce fichier contient les infos de température.\n",
    "\n",
    "Depuis un terminal :\n",
    "> zip -e x.zip x.csv  \n",
    "> rm x.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
